{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02827ea1-fd29-4b9c-b1c3-c5d97d64ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nikhi\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e33bd30-be05-4058-bef1-0b77b86b0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972a801a-c3c0-48ab-9949-67efb8a85d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('links.csv')\n",
    "movies = pd.read_csv('movies.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "tags = pd.read_csv('tags.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08608d93-96c7-4054-9576-d4b8efc5a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = ratings['userId'].unique()\n",
    "movie_ids = movies['movieId'].unique()\n",
    "tag_ids = tags['tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2904bf8-9aab-486b-bfbd-748c1416eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping = {id_: i for i, id_ in enumerate(user_ids)}\n",
    "movie_mapping = {id_: i for i, id_ in enumerate(movie_ids)}\n",
    "tag_mapping = {tag: i for i, tag in enumerate(tag_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695308f1-db91-485f-9323-32111e07cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['user_idx'] = ratings['userId'].map(user_mapping)\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802037e3-6629-4f43-a54a-c76928620fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperedges_ratings = ratings[['user_idx', 'movie_idx']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30eaa179-4959-4e08-be03-222920f7b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags['user_idx'] = tags['userId'].map(user_mapping)\n",
    "tags['movie_idx'] = tags['movieId'].map(movie_mapping)\n",
    "tags['tag_idx'] = tags['tag'].map(tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e02933-c4ed-4029-8f07-f2c35808a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperedges_tags = tags[['user_idx', 'movie_idx', 'tag_idx']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1a73762-0d54-454b-8ebb-f7e613a2916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = -1 * np.ones((hyperedges_ratings.shape[0], 1), dtype=np.int32)\n",
    "hyperedges_ratings_padded = np.hstack((hyperedges_ratings, padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84e442dc-95c0-48f5-80e6-3cadae142b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperedges = np.vstack((hyperedges_ratings_padded, hyperedges_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e914a69-7df8-4961-b91f-e3795c8e90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperedges = torch.tensor(hyperedges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0962f275-abbb-4683-85aa-becc366d1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(hyperedges))\n",
    "test_size = len(hyperedges) - train_size\n",
    "train_hyperedges, test_hyperedges = random_split(hyperedges, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8ecec33-da40-483b-a269-77abd0bafb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64  # Dimension of the embeddings\n",
    "num_users = len(user_mapping)\n",
    "num_movies = len(movie_mapping)\n",
    "num_tags = len(tag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "786cd583-029d-4b14-9d3a-5d6771a3e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "movie_embeddings = nn.Embedding(num_movies, embedding_dim)\n",
    "tag_embeddings = nn.Embedding(num_tags, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dfaf03d-0cb1-4f9a-bd0d-b8c41e5984d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0051, -0.0008,  0.0022,  ...,  0.0029,  0.0062, -0.0040],\n",
       "        [-0.0018, -0.0047, -0.0050,  ...,  0.0060,  0.0054,  0.0058],\n",
       "        [-0.0045, -0.0053,  0.0051,  ...,  0.0019,  0.0026,  0.0024],\n",
       "        ...,\n",
       "        [-0.0004,  0.0033,  0.0057,  ...,  0.0063, -0.0038,  0.0017],\n",
       "        [-0.0030, -0.0051,  0.0050,  ...,  0.0029, -0.0033, -0.0028],\n",
       "        [ 0.0064, -0.0021,  0.0027,  ...,  0.0064, -0.0049, -0.0033]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_uniform_(user_embeddings.weight)\n",
    "nn.init.xavier_uniform_(movie_embeddings.weight)\n",
    "nn.init.xavier_uniform_(tag_embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91d9d91a-229d-42ff-8e69-a7f061a912e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 200948\n",
      "Number of movies: 87585\n",
      "Number of tags: 140980\n",
      "Training hyperedges: 27200220\n",
      "Testing hyperedges: 6800056\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of movies: {num_movies}\")\n",
    "print(f\"Number of tags: {num_tags}\")\n",
    "print(f\"Training hyperedges: {len(train_hyperedges)}\")\n",
    "print(f\"Testing hyperedges: {len(test_hyperedges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a50fe997-5380-40b5-bfac-352d26f2099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_hyperedges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d91141aa-d07b-482f-ba42-4e8d2ea98b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 23030,   2305,     -1],\n",
      "        [ 81283,   1856,     -1],\n",
      "        [165781,    365,     -1],\n",
      "        [123403,   2245,     -1],\n",
      "        [ 17805,    133,     -1]])\n"
     ]
    }
   ],
   "source": [
    "print(train_hyperedges[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac7a345c-8fae-4768-8abe-f7565c2f6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_7900\\1637354247.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.hyperedges[idx])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HyperedgeDataset(Dataset):\n",
    "    def __init__(self, hyperedges):\n",
    "        self.hyperedges = hyperedges\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hyperedges)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.hyperedges[idx])\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = HyperedgeDataset(train_hyperedges)\n",
    "dataloader = DataLoader(dataset, batch_size=10000, shuffle=False)  # Adjust batch_size as needed\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch[batch[:, 2] == -1, 2] = 0\n",
    "    # Process the batch (e.g., update statistics or other operations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "738f1ec3-efd6-47a7-ad88-c060369688ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 200948\n",
      "Number of movies: 87585\n",
      "Number of tags: 140980\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'train_hyperedges' is a Subset object\n",
    "# Access the underlying dataset\n",
    "original_dataset = train_hyperedges.dataset\n",
    "\n",
    "# If 'original_dataset' is already a tensor, proceed with it\n",
    "# Otherwise, you might need to convert it to tensor as shown earlier\n",
    "\n",
    "# Extract data from the dataset and convert it to tensor if necessary\n",
    "# For illustration, let's assume 'original_dataset' has data in a list of lists format\n",
    "# Convert the data to tensor\n",
    "train_hyperedges_np = np.array(original_dataset)  # Adjust if necessary\n",
    "train_hyperedges = torch.tensor(train_hyperedges_np)\n",
    "\n",
    "# Replace -1 with 0\n",
    "train_hyperedges[train_hyperedges[:, 2] == -1, 2] = 0\n",
    "\n",
    "# Extract max values\n",
    "num_users = train_hyperedges[:, 0].max().item() + 1\n",
    "num_movies = train_hyperedges[:, 1].max().item() + 1\n",
    "num_tags = train_hyperedges[:, 2].max().item() + 1\n",
    "\n",
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of movies: {num_movies}\")\n",
    "print(f\"Number of tags: {num_tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c8010bc-fea7-46fd-b493-67552e11fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HypergraphContrastiveModel(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, num_tags, embed_dim):\n",
    "        super(HypergraphContrastiveModel, self).__init__()\n",
    "        # Define embedding layers\n",
    "        self.user_embeddings = nn.Embedding(num_users, embed_dim)\n",
    "        self.movie_embeddings = nn.Embedding(num_movies, embed_dim)\n",
    "        self.tag_embeddings = nn.Embedding(num_tags, embed_dim)\n",
    "        \n",
    "    def forward(self, hyperedges):\n",
    "        users = hyperedges[:, 0]\n",
    "        movies = hyperedges[:, 1]\n",
    "        tags = hyperedges[:, 2] if hyperedges.size(1) > 2 else None\n",
    "        \n",
    "        # Get embeddings\n",
    "        user_embeds = self.user_embeddings(users)\n",
    "        movie_embeds = self.movie_embeddings(movies)\n",
    "        tag_embeds = self.tag_embeddings(tags) if tags is not None else None\n",
    "        \n",
    "        # Debugging: print shapes of the embeddings\n",
    "        print(\"User embeddings shape:\", user_embeds.shape)\n",
    "        print(\"Movie embeddings shape:\", movie_embeds.shape)\n",
    "        if tag_embeds is not None:\n",
    "            print(\"Tag embeddings shape:\", tag_embeds.shape)\n",
    "        \n",
    "        return user_embeds, movie_embeds, tag_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70d230f2-a2d6-4a90-be22-fec31c5121f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(user_embeds, movie_embeds, tag_embeds, margin=1.0):\n",
    "    # Compute cosine similarity\n",
    "    user_movie_similarity = F.cosine_similarity(user_embeds.unsqueeze(1), movie_embeds.unsqueeze(0), dim=-1)\n",
    "    user_tag_similarity = F.cosine_similarity(user_embeds.unsqueeze(1), tag_embeds.unsqueeze(0), dim=-1)\n",
    "    \n",
    "    # Define loss as a combination of similarity scores\n",
    "    loss = F.mse_loss(user_movie_similarity, user_tag_similarity)\n",
    "    \n",
    "    return loss\n",
    "def validate(model, val_hyperedges):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        user_embeds, movie_embeds, tag_embeds = model(val_hyperedges)\n",
    "        val_loss = contrastive_loss(user_embeds, movie_embeds, tag_embeds)\n",
    "    print(f\"Validation Loss: {val_loss.item():.4f}\")\n",
    "    return val_loss.item()\n",
    "\n",
    "# Training Function with Early Stopping\n",
    "def train_contrastive_model(model, train_hyperedges, val_hyperedges, num_epochs=10, lr=0.001, batch_size=8, accumulation_steps=4, patience=3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i in range(0, len(train_hyperedges), batch_size):\n",
    "            hyperedge_batch = train_hyperedges[i:i+batch_size]\n",
    "            \n",
    "            with torch.cuda.amp.autocast():  # Mixed precision context\n",
    "                user_embeds, movie_embeds, tag_embeds = model(hyperedge_batch)\n",
    "                loss = contrastive_loss(user_embeds, movie_embeds, tag_embeds)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i // batch_size + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss = validate(model, val_hyperedges)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping due to no improvement.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6faacfb9-fd4b-401f-bc43-10928917e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "Epoch [1/10], Loss: 0.0160\n",
      "User embeddings shape: torch.Size([16, 128])\n",
      "Movie embeddings shape: torch.Size([16, 128])\n",
      "Tag embeddings shape: torch.Size([16, 128])\n",
      "Validation Loss: 0.0151\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "Epoch [2/10], Loss: 0.0157\n",
      "User embeddings shape: torch.Size([16, 128])\n",
      "Movie embeddings shape: torch.Size([16, 128])\n",
      "Tag embeddings shape: torch.Size([16, 128])\n",
      "Validation Loss: 0.0151\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "Epoch [3/10], Loss: 0.0155\n",
      "User embeddings shape: torch.Size([16, 128])\n",
      "Movie embeddings shape: torch.Size([16, 128])\n",
      "Tag embeddings shape: torch.Size([16, 128])\n",
      "Validation Loss: 0.0151\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "User embeddings shape: torch.Size([8, 128])\n",
      "Movie embeddings shape: torch.Size([8, 128])\n",
      "Tag embeddings shape: torch.Size([8, 128])\n",
      "Epoch [4/10], Loss: 0.0153\n",
      "User embeddings shape: torch.Size([16, 128])\n",
      "Movie embeddings shape: torch.Size([16, 128])\n",
      "Tag embeddings shape: torch.Size([16, 128])\n",
      "Validation Loss: 0.0151\n",
      "Early stopping due to no improvement.\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128  # Adjust as needed\n",
    "\n",
    "model = HypergraphContrastiveModel(num_users, num_movies, num_tags, embed_dim)\n",
    "\n",
    "# Example train and validation hyperedges tensors with reduced batch size\n",
    "train_hyperedges = torch.randint(0, min(num_users, num_movies, num_tags), (32, 3))  # Simulated smaller batch size\n",
    "val_hyperedges = torch.randint(0, min(num_users, num_movies, num_tags), (16, 3))  # Simulated validation set\n",
    "\n",
    "# Train the model with validation and early stopping\n",
    "train_contrastive_model(model, train_hyperedges, val_hyperedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e946fec9-b5b1-4e85-8b07-67f5c0005f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User embeddings shape: torch.Size([16, 128])\n",
      "Movie embeddings shape: torch.Size([16, 128])\n",
      "Tag embeddings shape: torch.Size([16, 128])\n",
      "Accuracy: 0.8750\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.875, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def predict_labels(user_embeds, movie_embeds, threshold=0.5):\n",
    "    # Compute cosine similarity between user and movie embeddings\n",
    "    similarity = torch.cosine_similarity(user_embeds, movie_embeds, dim=-1)\n",
    "    \n",
    "    # Predict label based on the threshold\n",
    "    predictions = (similarity > threshold).int()  # 1 if similarity > threshold, else 0\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(model, val_hyperedges, true_labels, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_embeds, movie_embeds, tag_embeds = model(val_hyperedges)\n",
    "        predictions = predict_labels(user_embeds, movie_embeds, threshold)\n",
    "    \n",
    "    # Convert tensors to numpy arrays for compatibility with sklearn\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    true_labels = true_labels.cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Example usage\n",
    "# true_labels should be the ground truth labels for your validation set\n",
    "true_labels = torch.randint(0, 2, (val_hyperedges.size(0),))  # Simulated ground truth labels\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "evaluate_model(model, val_hyperedges, true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e36165d9-ed31-485d-9917-b2a716b1a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(user_embeddings, movie_embeddings, threshold=0.5):\n",
    "    # Compute cosine similarity between user and movie embeddings\n",
    "    similarity = torch.cosine_similarity(user_embeddings, movie_embeddings, dim=-1)\n",
    "    print(f\"Similarity stats: min={similarity.min().item()}, max={similarity.max().item()}, mean={similarity.mean().item()}\")\n",
    "    # Predict label based on the threshold\n",
    "    predictions = (similarity > threshold).int()  # 1 if similarity > threshold, else 0\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2c54504-85ab-4bc3-b33a-b79c38b05d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, true_labels, thresholds):\n",
    "    # Convert predictions and true_labels to numpy arrays or use PyTorch metrics\n",
    "    predictions = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    true_labels = true_labels.numpy() if isinstance(true_labels, torch.Tensor) else true_labels\n",
    "    \n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Binarize predictions based on the threshold\n",
    "        binarized_predictions = (predictions > threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy.append((binarized_predictions == true_labels).mean())\n",
    "        tp = ((binarized_predictions == 1) & (true_labels == 1)).sum()\n",
    "        fp = ((binarized_predictions == 1) & (true_labels == 0)).sum()\n",
    "        fn = ((binarized_predictions == 0) & (true_labels == 1)).sum()\n",
    "\n",
    "        precision.append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        recall.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        f1.append((2 * precision[-1] * recall[-1]) / (precision[-1] + recall[-1]) if (precision[-1] + recall[-1]) > 0 else 0)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7b98a579-23fe-4c3f-b5d4-41183ba32bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Similarity stats: min=-41.15969467163086, max=36.23440933227539, mean=0.3133211135864258\n",
      "Threshold: -0.1, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1 Score: 0.6667\n",
      "Adjusted Similarity stats: min=-41.15969467163086, max=36.23440933227539, mean=0.3133211135864258\n",
      "Threshold: 0.0, Accuracy: 0.4336, Precision: 0.3664, Recall: 0.4364, F1 Score: 0.3983\n",
      "Adjusted Similarity stats: min=-41.15969467163086, max=36.23440933227539, mean=0.3133211135864258\n",
      "Threshold: 0.1, Accuracy: 0.5430, Precision: 0.5538, Recall: 0.5496, F1 Score: 0.5517\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define or import the evaluate_model function\n",
    "def evaluate_model(predictions, true_labels, thresholds):\n",
    "    # Convert predictions and true_labels to numpy arrays or use PyTorch metrics\n",
    "    predictions = predictions.numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "    true_labels = true_labels.numpy() if isinstance(true_labels, torch.Tensor) else true_labels\n",
    "    \n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Binarize predictions based on the threshold\n",
    "        binarized_predictions = (predictions > threshold).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy.append((binarized_predictions == true_labels).mean())\n",
    "        tp = ((binarized_predictions == 1) & (true_labels == 1)).sum()\n",
    "        fp = ((binarized_predictions == 1) & (true_labels == 0)).sum()\n",
    "        fn = ((binarized_predictions == 0) & (true_labels == 1)).sum()\n",
    "\n",
    "        precision.append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "        recall.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        f1.append((2 * precision[-1] * recall[-1]) / (precision[-1] + recall[-1]) if (precision[-1] + recall[-1]) > 0 else 0)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Function to predict labels\n",
    "def predict_labels(user_indices, movie_indices, model, threshold=0.0):\n",
    "    user_embeddings, movie_embeddings, _ = model(user_indices, movie_indices)\n",
    "    similarity = torch.mm(user_embeddings, movie_embeddings.t())  # (batch_size, batch_size)\n",
    "    print(f\"Adjusted Similarity stats: min={similarity.min().item()}, max={similarity.max().item()}, mean={similarity.mean().item()}\")\n",
    "    predictions = (similarity > threshold).int()\n",
    "    return predictions\n",
    "\n",
    "# Example validation indices\n",
    "val_user_indices = torch.arange(16)\n",
    "val_movie_indices = torch.arange(16)\n",
    "\n",
    "# Example model\n",
    "model = HypergraphContrastiveModel(num_users, num_movies, num_tags, embed_dim=128)\n",
    "\n",
    "# Example usage with adjusted threshold\n",
    "for threshold in [-0.1, 0.0, 0.1]:\n",
    "    predictions = predict_labels(val_user_indices, val_movie_indices, model, threshold)\n",
    "    # Assume true_labels is defined and corresponds to the same shape as predictions\n",
    "    true_labels = torch.randint(0, 2, (16, 16))  # Sample true labels for demonstration\n",
    "    accuracy, precision, recall, f1 = evaluate_model(predictions, true_labels, [threshold])\n",
    "    print(f\"Threshold: {threshold}, Accuracy: {accuracy[0]:.4f}, Precision: {precision[0]:.4f}, Recall: {recall[0]:.4f}, F1 Score: {f1[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "28442737-281f-47bb-af58-0dea397498ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.001325011253357\n",
      "Epoch 2/10, Loss: 0.9710863828659058\n",
      "Epoch 3/10, Loss: 0.9536166787147522\n",
      "Epoch 4/10, Loss: 0.9417213797569275\n",
      "Epoch 5/10, Loss: 0.910735011100769\n",
      "Epoch 6/10, Loss: 0.867866039276123\n",
      "Epoch 7/10, Loss: 0.8692584037780762\n",
      "Epoch 8/10, Loss: 0.8301055431365967\n",
      "Epoch 9/10, Loss: 0.8074963688850403\n",
      "Epoch 10/10, Loss: 0.807449221611023\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ImprovedHypergraphContrastiveModel(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, num_tags, embed_dim):\n",
    "        super(ImprovedHypergraphContrastiveModel, self).__init__()\n",
    "        self.user_embeddings = nn.Embedding(num_users, embed_dim)\n",
    "        self.movie_embeddings = nn.Embedding(num_movies, embed_dim)\n",
    "        self.tag_embeddings = nn.Embedding(num_tags, embed_dim)\n",
    "        \n",
    "        # Additional layers\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, hyperedges):\n",
    "        users = hyperedges[:, 0]\n",
    "        movies = hyperedges[:, 1]\n",
    "        tags = hyperedges[:, 2] if hyperedges.size(1) > 2 else None\n",
    "        \n",
    "        user_embeds = self.user_embeddings(users)\n",
    "        movie_embeds = self.movie_embeddings(movies)\n",
    "        tag_embeds = self.tag_embeddings(tags) if tags is not None else None\n",
    "        \n",
    "        # Additional processing\n",
    "        user_embeds = self.fc(user_embeds)\n",
    "        movie_embeds = self.fc(movie_embeds)\n",
    "        user_embeds = self.dropout(user_embeds)\n",
    "        movie_embeds = self.dropout(movie_embeds)\n",
    "        \n",
    "        return user_embeds, movie_embeds, tag_embeds\n",
    "\n",
    "def train_improved_model(model, train_hyperedges, num_epochs=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()  # Example loss function, adjust as needed\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        user_embeds, movie_embeds, _ = model(train_hyperedges)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = contrastive_loss(user_embeds, movie_embeds)  # Define contrastive_loss as needed\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "def contrastive_loss(user_embeds, movie_embeds, margin=1.0):\n",
    "    # Define contrastive loss function\n",
    "    user_movie_similarity = torch.cosine_similarity(user_embeds, movie_embeds)\n",
    "    loss = torch.mean(torch.clamp(margin - user_movie_similarity, min=0.0))\n",
    "    return loss\n",
    "\n",
    "# Example usage with improved model\n",
    "embed_dim = 256  # Try different embedding sizes\n",
    "model = ImprovedHypergraphContrastiveModel(num_users, num_movies, num_tags, embed_dim)\n",
    "train_improved_model(model, train_hyperedges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a72693b2-9b1d-46d6-bb9a-677bbff397fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyperedges_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[indices]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get validation data\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m val_user_indices, val_movie_indices, val_true_labels \u001b[38;5;241m=\u001b[39m get_indices_and_labels(subset_to_tensor(test_hyperedges, hyperedges_tensor))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Now, you can evaluate your model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_user_indices, val_movie_indices, val_true_labels, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hyperedges_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "def get_indices_and_labels(hyperedges):\n",
    "    # Assuming the hyperedges tensor has shape [N, 3]\n",
    "    user_indices = hyperedges[:, 0]\n",
    "    movie_indices = hyperedges[:, 1]\n",
    "    true_labels = hyperedges[:, 2]\n",
    "    return user_indices, movie_indices, true_labels\n",
    "\n",
    "# Convert Subset objects to tensors if needed\n",
    "def subset_to_tensor(subset, data):\n",
    "    indices = subset.indices\n",
    "    return data[indices]\n",
    "\n",
    "# Get validation data\n",
    "val_user_indices, val_movie_indices, val_true_labels = get_indices_and_labels(subset_to_tensor(test_hyperedges, hyperedges_tensor))\n",
    "\n",
    "# Now, you can evaluate your model\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, val_user_indices, val_movie_indices, val_true_labels, threshold=0.1)\n",
    "print(f\"Threshold: 0.1, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
